{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NeuroSHAP: Network-Aware SHAP for Neuroimaging Data\n",
    "### Proof-of-Concept Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "import shap\n",
    "from nilearn import datasets\n",
    "from nilearn.connectome import ConnectivityMeasure\n",
    "from nilearn.maskers import NiftiLabelsMasker\n",
    "from nilearn.datasets import fetch_development_fmri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Forrest Gump dataset...\n",
      "[fetch_development_fmri] Dataset found in /Users/henrikformoe/nilearn_data/development_fmri\n",
      "[fetch_development_fmri] Dataset found in /Users/henrikformoe/nilearn_data/development_fmri/development_fmri\n",
      "[fetch_development_fmri] Dataset found in /Users/henrikformoe/nilearn_data/development_fmri/development_fmri\n",
      "[fetch_atlas_yeo_2011] Dataset found in /Users/henrikformoe/nilearn_data/yeo_2011\n",
      "Extracting time series...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/kb/t8njst5d1qg_hr3b0_jq9s2r0000gn/T/ipykernel_74233/48396186.py:5: DeprecationWarning: From release >=0.13.0, instead of returning several atlas image accessible via different keys, this fetcher will return the atlas as a dictionary with a single atlas image, accessible through a 'maps' key. To suppress this warning, Please use the parameters 'n_networks' and 'thickness' to specify the exact atlas image you want.\n",
      "  yeo_atlas = datasets.fetch_atlas_yeo_2011()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed subject 1/100\n",
      "Processed subject 2/100\n",
      "Processed subject 3/100\n",
      "Processed subject 4/100\n",
      "Processed subject 5/100\n",
      "Processed subject 6/100\n",
      "Processed subject 7/100\n",
      "Processed subject 8/100\n",
      "Processed subject 9/100\n",
      "Processed subject 10/100\n",
      "Processed subject 11/100\n",
      "Processed subject 12/100\n",
      "Processed subject 13/100\n",
      "Processed subject 14/100\n",
      "Processed subject 15/100\n",
      "Processed subject 16/100\n",
      "Processed subject 17/100\n",
      "Processed subject 18/100\n",
      "Processed subject 19/100\n",
      "Processed subject 20/100\n",
      "Processed subject 21/100\n",
      "Processed subject 22/100\n",
      "Processed subject 23/100\n",
      "Processed subject 24/100\n",
      "Processed subject 25/100\n",
      "Processed subject 26/100\n",
      "Processed subject 27/100\n",
      "Processed subject 28/100\n",
      "Processed subject 29/100\n",
      "Processed subject 30/100\n",
      "Processed subject 31/100\n",
      "Processed subject 32/100\n",
      "Processed subject 33/100\n",
      "Processed subject 34/100\n",
      "Processed subject 35/100\n",
      "Processed subject 36/100\n",
      "Processed subject 37/100\n",
      "Processed subject 38/100\n",
      "Processed subject 39/100\n",
      "Processed subject 40/100\n",
      "Processed subject 41/100\n",
      "Processed subject 42/100\n",
      "Processed subject 43/100\n",
      "Processed subject 44/100\n",
      "Processed subject 45/100\n",
      "Processed subject 46/100\n",
      "Processed subject 47/100\n",
      "Processed subject 48/100\n",
      "Processed subject 49/100\n",
      "Processed subject 50/100\n",
      "Processed subject 51/100\n",
      "Processed subject 52/100\n",
      "Processed subject 53/100\n",
      "Processed subject 54/100\n",
      "Processed subject 55/100\n",
      "Processed subject 56/100\n",
      "Processed subject 57/100\n",
      "Processed subject 58/100\n",
      "Processed subject 59/100\n",
      "Processed subject 60/100\n",
      "Processed subject 61/100\n",
      "Processed subject 62/100\n",
      "Processed subject 63/100\n",
      "Processed subject 64/100\n",
      "Processed subject 65/100\n",
      "Processed subject 66/100\n",
      "Processed subject 67/100\n",
      "Processed subject 68/100\n",
      "Processed subject 69/100\n",
      "Processed subject 70/100\n",
      "Processed subject 71/100\n",
      "Processed subject 72/100\n",
      "Processed subject 73/100\n",
      "Processed subject 74/100\n",
      "Processed subject 75/100\n",
      "Processed subject 76/100\n",
      "Processed subject 77/100\n",
      "Processed subject 78/100\n",
      "Processed subject 79/100\n",
      "Processed subject 80/100\n",
      "Processed subject 81/100\n",
      "Processed subject 82/100\n",
      "Processed subject 83/100\n",
      "Processed subject 84/100\n",
      "Processed subject 85/100\n",
      "Processed subject 86/100\n",
      "Processed subject 87/100\n",
      "Processed subject 88/100\n",
      "Processed subject 89/100\n",
      "Processed subject 90/100\n",
      "Processed subject 91/100\n",
      "Processed subject 92/100\n",
      "Processed subject 93/100\n",
      "Processed subject 94/100\n",
      "Processed subject 95/100\n",
      "Processed subject 96/100\n",
      "Processed subject 97/100\n",
      "Processed subject 98/100\n",
      "Processed subject 99/100\n",
      "Processed subject 100/100\n",
      "Number of subject ages collected: 100\n",
      "Computing functional connectivity...\n",
      "Connectivity matrices shape: (100, 7, 7)\n",
      "Using 100 subjects with valid features\n",
      "Data prepared: 100 subjects, 21 connectivity features\n",
      "X shape: (100, 21), y shape: (100,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading Forrest Gump dataset...\")\n",
    "# Load Forrest Gump dataset\n",
    "forrest_gump = fetch_development_fmri(n_subjects=100)\n",
    "# Load Yeo 7-network atlas\n",
    "yeo_atlas = datasets.fetch_atlas_yeo_2011()\n",
    "atlas_filename = yeo_atlas['thick_7']\n",
    "print(\"Extracting time series...\")\n",
    "# Create a masker to extract time series from each network\n",
    "masker = NiftiLabelsMasker(\n",
    "    labels_img=atlas_filename,\n",
    "    standardize=True,\n",
    "    memory='nilearn_cache',\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "# Extract time series for each subject\n",
    "subject_data = []\n",
    "subject_ids = []\n",
    "subject_ages = []\n",
    "\n",
    "# Process each subject - using direct indexing to ensure matching\n",
    "for i, func_file in enumerate(forrest_gump.func):\n",
    "    try:\n",
    "        # Extract time series from the atlas regions\n",
    "        time_series = masker.fit_transform(func_file)\n",
    "        \n",
    "        # Get age data - using direct iloc indexing which works with the dataset\n",
    "        if i < len(forrest_gump.phenotypic):\n",
    "            subject_age = forrest_gump.phenotypic.iloc[i]['Age']\n",
    "            subject_data.append(time_series)\n",
    "            subject_ids.append(i)\n",
    "            subject_ages.append(subject_age)\n",
    "            print(f\"Processed subject {i+1}/{len(forrest_gump.func)}\")\n",
    "        else:\n",
    "            print(f\"Skipping subject {i+1} - no matching phenotypic data\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing subject {i}: {e}\")\n",
    "\n",
    "print(f\"Number of subject ages collected: {len(subject_ages)}\")\n",
    "\n",
    "# Convert ages to binary classification (younger vs. older)\n",
    "subject_ages = np.array(subject_ages)\n",
    "age_median = np.median(subject_ages)\n",
    "age_labels = (subject_ages > age_median).astype(int)  # 0: younger, 1: older\n",
    "\n",
    "# Convert to numpy arrays\n",
    "subject_data = np.array(subject_data)\n",
    "\n",
    "# Compute functional connectivity (correlation) for each subject\n",
    "print(\"Computing functional connectivity...\")\n",
    "correlation_measure = ConnectivityMeasure(kind='correlation')\n",
    "connectivity_matrices = correlation_measure.fit_transform(subject_data)\n",
    "print(f\"Connectivity matrices shape: {connectivity_matrices.shape}\")\n",
    "\n",
    "# Extract the upper triangle of each connectivity matrix as features\n",
    "def extract_upper_triangle(matrix):\n",
    "    num_networks = matrix.shape[0]\n",
    "    indices = np.triu_indices(num_networks, k=1)\n",
    "    return matrix[indices]\n",
    "\n",
    "# Extract features with error checking\n",
    "X = []\n",
    "valid_indices = []\n",
    "for i, matrix in enumerate(connectivity_matrices):\n",
    "    features = extract_upper_triangle(matrix)\n",
    "    if not np.isnan(features).any():  # Check if features contain NaN\n",
    "        X.append(features)\n",
    "        valid_indices.append(i)\n",
    "    else:\n",
    "        print(f\"Subject {i}: Extracted features contain NaN values\")\n",
    "\n",
    "X = np.array(X)\n",
    "valid_indices = np.array(valid_indices)\n",
    "\n",
    "# Use valid indices for both X and y\n",
    "y = age_labels[valid_indices]\n",
    "print(f\"Using {len(valid_indices)} subjects with valid features\")\n",
    "\n",
    "print(f\"Data prepared: {X.shape[0]} subjects, {X.shape[1]} connectivity features\")\n",
    "print(f\"X shape: {X.shape}, y shape: {y.shape}\")\n",
    "\n",
    "# Final validation - make absolutely sure X and y match\n",
    "assert X.shape[0] == y.shape[0], \"Mismatch between X and y shapes\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Logistic Regression model with cross-validation...\n",
      "Original data shapes - X: (100, 21), y: (100,)\n",
      "Final data shapes - X: (100, 21), y: (100,)\n",
      "Using 2-fold cross-validation\n",
      "Model accuracy on full dataset: 0.80\n",
      "Cross-validation accuracy: 0.62 ± 0.08\n"
     ]
    }
   ],
   "source": [
    "# 2. Model Training with Logistic Regression and Cross-Validation\n",
    "# ------------------------------------------\n",
    "print(\"Training Logistic Regression model with cross-validation...\")\n",
    "\n",
    "# Print shape information to debug\n",
    "print(f\"Original data shapes - X: {X.shape}, y: {y.shape}\")\n",
    "\n",
    "# Check for and remove any NaN values\n",
    "nan_mask_X = np.isnan(X).any(axis=1)\n",
    "if nan_mask_X.any():\n",
    "    print(f\"Found {np.sum(nan_mask_X)} rows with NaN values in X\")\n",
    "    X = X[~nan_mask_X]\n",
    "    y = y[~nan_mask_X]\n",
    "\n",
    "# Make sure X and y have the same number of samples\n",
    "if len(X) != len(y):\n",
    "    print(f\"Mismatch in sample count: X has {len(X)} samples, y has {len(y)} samples\")\n",
    "    # Keep only the samples that have both X and y data\n",
    "    min_samples = min(len(X), len(y))\n",
    "    X = X[:min_samples]\n",
    "    y = y[:min_samples]\n",
    "    print(f\"Adjusted to {min_samples} samples\")\n",
    "\n",
    "# Scale features (important for logistic regression)\n",
    "print(f\"Final data shapes - X: {X.shape}, y: {y.shape}\")\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Use L2 regularization with reasonable C value\n",
    "model = LogisticRegression(C=1.0, solver='liblinear', random_state=42)\n",
    "\n",
    "# For cross-validation, adjust n_splits if sample size is small\n",
    "n_splits = min(5, len(np.unique(y)))  # Ensure we don't have more splits than classes\n",
    "print(f\"Using {n_splits}-fold cross-validation\")\n",
    "cv = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "# Run cross-validation\n",
    "cv_scores = cross_val_score(model, X_scaled, y, cv=cv, scoring='accuracy')\n",
    "\n",
    "# Train final model on all data\n",
    "model.fit(X_scaled, y)\n",
    "\n",
    "# Print results\n",
    "print(f\"Model accuracy on full dataset: {model.score(X_scaled, y):.2f}\")\n",
    "print(f\"Cross-validation accuracy: {cv_scores.mean():.2f} ± {cv_scores.std():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing standard SHAP values...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bada9ecb7e244e509b2f247ac7899e41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 3. Standard SHAP Implementation\n",
    "# ------------------------------\n",
    "print(\"Computing standard SHAP values...\")\n",
    "# Create a prediction function that returns probabilities for class 1\n",
    "def predict_proba_class1(X):\n",
    "    return model.predict_proba(X)[:, 1]\n",
    "\n",
    "# Create explainer with scaled data\n",
    "explainer = shap.KernelExplainer(predict_proba_class1, X_scaled)\n",
    "standard_shap_values = explainer.shap_values(X_scaled[0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing network-aware SHAP values...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78aac09534f34cc49c0b1f5fad645558",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 4. Network-Aware SHAP Implementation\n",
    "# -----------------------------------\n",
    "print(\"Computing network-aware SHAP values...\")\n",
    "\n",
    "# Generate network-aware background data (using scaled data)\n",
    "def network_aware_background_sampler(X, background_size=100):\n",
    "    # Calculate the mean and covariance of the feature matrix\n",
    "    mean_vec = np.mean(X, axis=0)\n",
    "    cov_matrix = np.cov(X, rowvar=False)\n",
    "    \n",
    "    # Generate new samples using the covariance structure\n",
    "    network_aware_background = np.random.multivariate_normal(\n",
    "        mean=mean_vec, \n",
    "        cov=cov_matrix, \n",
    "        size=background_size\n",
    "    )\n",
    "    \n",
    "    return network_aware_background\n",
    "\n",
    "# Generate network-aware background data\n",
    "network_background = network_aware_background_sampler(X_scaled)\n",
    "\n",
    "# Create network-aware explainer\n",
    "network_explainer = shap.KernelExplainer(\n",
    "    predict_proba_class1,\n",
    "    network_background\n",
    ")\n",
    "\n",
    "# Calculate network-aware SHAP values\n",
    "network_shap_values = network_explainer.shap_values(X_scaled[0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Visualization and Comparison\n",
    "# ------------------------------\n",
    "print(\"Creating visualizations...\")\n",
    "\n",
    "# Get network names\n",
    "n_networks = 7\n",
    "network_names = ['Visual', 'Somatomotor', 'DorsalAttn', 'VentralAttn', \n",
    "                'Limbic', 'Frontoparietal', 'Default']\n",
    "\n",
    "# Initialize network-level SHAP values\n",
    "standard_network_shap = np.zeros(n_networks)\n",
    "network_aware_network_shap = np.zeros(n_networks)\n",
    "\n",
    "# Map edge SHAP values back to networks\n",
    "feature_idx = 0\n",
    "for i in range(n_networks):\n",
    "    for j in range(i+1, n_networks):\n",
    "        # Add edge contribution to both connected networks\n",
    "        standard_network_shap[i] += standard_shap_values[0][feature_idx] / 2\n",
    "        standard_network_shap[j] += standard_shap_values[0][feature_idx] / 2\n",
    "        \n",
    "        network_aware_network_shap[i] += network_shap_values[0][feature_idx] / 2\n",
    "        network_aware_network_shap[j] += network_shap_values[0][feature_idx] / 2\n",
    "        \n",
    "        feature_idx += 1\n",
    "\n",
    "# Create side-by-side visualization\n",
    "plt.figure(figsize=(15, 6))\n",
    "\n",
    "# Standard SHAP visualization\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.barh(network_names, standard_network_shap, color='cornflowerblue')\n",
    "plt.title('Standard SHAP Values')\n",
    "plt.xlabel('SHAP Value')\n",
    "plt.axvline(x=0, color='black', linestyle='-', alpha=0.3)\n",
    "\n",
    "# Network-aware SHAP visualization\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.barh(network_names, network_aware_network_shap, color='indianred')\n",
    "plt.title('Network-Aware SHAP (NeuroSHAP)')\n",
    "plt.xlabel('SHAP Value')\n",
    "plt.axvline(x=0, color='black', linestyle='-', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('neuroshap_logistic_comparison.png')\n",
    "print(\"Visualization saved as 'neuroshap_logistic_comparison.png'\")\n",
    "\n",
    "# Display key findings\n",
    "print(\"\\nKey differences between methods:\")\n",
    "for i, network in enumerate(network_names):\n",
    "    diff = network_aware_network_shap[i] - standard_network_shap[i]\n",
    "    if abs(diff) > 0.01:\n",
    "        print(f\"{network}: {'Increased' if diff > 0 else 'Decreased'} importance \" +\n",
    "              f\"by {abs(diff):.3f} with network-aware approach\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced Network-Aware SHAP Implementation\n",
    "# ------------------------------------------\n",
    "\n",
    "# 1. Add visualization of correlation structure\n",
    "print(\"Visualizing network connectivity structure...\")\n",
    "plt.figure(figsize=(10, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(np.corrcoef(X_scaled.T), cmap='coolwarm', vmin=-1, vmax=1)\n",
    "plt.colorbar(label='Pearson r')\n",
    "plt.title('Feature Correlation Structure')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(np.corrcoef(network_background.T), cmap='coolwarm', vmin=-1, vmax=1)\n",
    "plt.colorbar(label='Pearson r')\n",
    "plt.title('Network-Aware Background Correlation')\n",
    "plt.tight_layout()\n",
    "plt.savefig('network_structure_comparison.png')\n",
    "\n",
    "# 2. Improved network-level SHAP visualization\n",
    "print(\"Creating enhanced network visualizations...\")\n",
    "\n",
    "# Function to map features to network pairs\n",
    "def get_network_pair_indices(n_networks):\n",
    "    pairs = []\n",
    "    idx = 0\n",
    "    for i in range(n_networks):\n",
    "        for j in range(i+1, n_networks):\n",
    "            pairs.append((idx, (i, j)))\n",
    "            idx += 1\n",
    "    return pairs\n",
    "\n",
    "# Get network pair mapping\n",
    "network_pairs = get_network_pair_indices(n_networks)\n",
    "\n",
    "# Create a network x network SHAP matrix visualization\n",
    "def create_network_matrix(shap_values, n_networks, network_pairs):\n",
    "    matrix = np.zeros((n_networks, n_networks))\n",
    "    for idx, (i, j) in network_pairs:\n",
    "        matrix[i, j] = shap_values[0][idx]\n",
    "        matrix[j, i] = shap_values[0][idx]  # Mirror for visualization\n",
    "    return matrix\n",
    "\n",
    "# Visualize SHAP values as connectivity matrices\n",
    "plt.figure(figsize=(15, 6))\n",
    "\n",
    "# Standard SHAP matrix\n",
    "std_matrix = create_network_matrix(standard_shap_values, n_networks, network_pairs)\n",
    "plt.subplot(1, 2, 1)\n",
    "im = plt.imshow(std_matrix, cmap='RdBu_r', vmin=-np.max(abs(std_matrix)), vmax=np.max(abs(std_matrix)))\n",
    "plt.colorbar(im, label='SHAP Value')\n",
    "plt.title('Standard SHAP Network Contributions')\n",
    "plt.xticks(range(n_networks), network_names, rotation=45, ha='right')\n",
    "plt.yticks(range(n_networks), network_names)\n",
    "\n",
    "# Network-aware SHAP matrix\n",
    "net_matrix = create_network_matrix(network_shap_values, n_networks, network_pairs)\n",
    "plt.subplot(1, 2, 2)\n",
    "im = plt.imshow(net_matrix, cmap='RdBu_r', vmin=-np.max(abs(net_matrix)), vmax=np.max(abs(net_matrix)))\n",
    "plt.colorbar(im, label='SHAP Value')\n",
    "plt.title('Network-Aware SHAP Network Contributions')\n",
    "plt.xticks(range(n_networks), network_names, rotation=45, ha='right')\n",
    "plt.yticks(range(n_networks), network_names)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('neuroshap_matrix_comparison.png')\n",
    "\n",
    "# 3. Analyze stability with different background samples\n",
    "print(\"Testing stability of network-aware SHAP...\")\n",
    "background_sizes = [50, 100, 200]\n",
    "stability_results = []\n",
    "\n",
    "for bg_size in background_sizes:\n",
    "    # Generate different background samples\n",
    "    bg_sample = network_aware_background_sampler(X_scaled, background_size=bg_size)\n",
    "    temp_explainer = shap.KernelExplainer(predict_proba_class1, bg_sample)\n",
    "    temp_shap = temp_explainer.shap_values(X_scaled[0:1])\n",
    "    \n",
    "    # Map to network level\n",
    "    network_shap = np.zeros(n_networks)\n",
    "    feature_idx = 0\n",
    "    for i in range(n_networks):\n",
    "        for j in range(i+1, n_networks):\n",
    "            network_shap[i] += temp_shap[0][feature_idx] / 2\n",
    "            network_shap[j] += temp_shap[0][feature_idx] / 2\n",
    "            feature_idx += 1\n",
    "    \n",
    "    stability_results.append(network_shap)\n",
    "\n",
    "# Visualize stability\n",
    "plt.figure(figsize=(10, 6))\n",
    "for i, size in enumerate(background_sizes):\n",
    "    plt.plot(range(n_networks), stability_results[i], 'o-', label=f'Background size={size}')\n",
    "plt.xticks(range(n_networks), network_names, rotation=45, ha='right')\n",
    "plt.legend()\n",
    "plt.title('Stability of Network-Aware SHAP Values')\n",
    "plt.ylabel('Network SHAP Value')\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('neuroshap_stability.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neuroshap_s25_v1_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
